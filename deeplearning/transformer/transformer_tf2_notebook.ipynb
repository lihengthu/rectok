{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    return pos * i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "\n",
    "    # apply sin to even indices in the array: 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array: 2i + 1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 5\n",
    "d_model = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(position)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(d_model)[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_rads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
       "       [ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30],\n",
       "       [ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45],\n",
       "       [ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_rads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  2,  4,  6,  8, 10, 12, 14],\n",
       "       [ 0,  4,  8, 12, 16, 20, 24, 28],\n",
       "       [ 0,  6, 12, 18, 24, 30, 36, 42],\n",
       "       [ 0,  8, 16, 24, 32, 40, 48, 56]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_rads[:, 0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  7,  9, 11, 13, 15],\n",
       "       [ 2,  6, 10, 14, 18, 22, 26, 30],\n",
       "       [ 3,  9, 15, 21, 27, 33, 39, 45],\n",
       "       [ 4, 12, 20, 28, 36, 44, 52, 60]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_rads[:, 1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 16), dtype=float32, numpy=\n",
       "array([[[ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00,  1.0000000e+00,\n",
       "          0.0000000e+00,  1.0000000e+00,  0.0000000e+00,  1.0000000e+00,\n",
       "          0.0000000e+00,  1.0000000e+00,  0.0000000e+00,  1.0000000e+00,\n",
       "          0.0000000e+00,  1.0000000e+00,  0.0000000e+00,  1.0000000e+00],\n",
       "        [ 8.4147096e-01,  5.4030228e-01,  3.1098360e-01,  9.5041525e-01,\n",
       "          9.9833414e-02,  9.9500418e-01,  3.1617507e-02,  9.9950004e-01,\n",
       "          9.9998331e-03,  9.9994999e-01,  3.1622723e-03,  9.9999499e-01,\n",
       "          9.9999981e-04,  9.9999952e-01,  3.1622776e-04,  9.9999994e-01],\n",
       "        [ 9.0929741e-01, -4.1614684e-01,  5.9112710e-01,  8.0657840e-01,\n",
       "          1.9866933e-01,  9.8006660e-01,  6.3203394e-02,  9.9800068e-01,\n",
       "          1.9998666e-02,  9.9980003e-01,  6.3245133e-03,  9.9997997e-01,\n",
       "          1.9999987e-03,  9.9999797e-01,  6.3245551e-04,  9.9999982e-01],\n",
       "        [ 1.4112000e-01, -9.8999250e-01,  8.1264889e-01,  5.8275360e-01,\n",
       "          2.9552022e-01,  9.5533651e-01,  9.4726093e-02,  9.9550337e-01,\n",
       "          2.9995501e-02,  9.9955004e-01,  9.4866911e-03,  9.9995500e-01,\n",
       "          2.9999956e-03,  9.9999553e-01,  9.4868318e-04,  9.9999952e-01],\n",
       "        [-7.5680250e-01, -6.5364361e-01,  9.5358074e-01,  3.0113748e-01,\n",
       "          3.8941833e-01,  9.2106098e-01,  1.2615407e-01,  9.9201065e-01,\n",
       "          3.9989334e-02,  9.9920011e-01,  1.2648773e-02,  9.9992001e-01,\n",
       "          3.9999895e-03,  9.9999201e-01,  1.2649107e-03,  9.9999923e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(position, d_model)\n",
    "pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 16])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pos_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, d_model))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # add extra dimensions to add the padding to the attention logits\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=int32, numpy=\n",
       "array([[7, 6, 0, 0, 1],\n",
       "       [1, 2, 3, 0, 0],\n",
       "       [0, 0, 0, 4, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [1., 1., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.math.equal(x, 0), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 1, 1, 5])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[7, 6, 9],\n",
       "       [1, 2, 3],\n",
       "       [0, 4, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 9], [1, 2, 3], [0, 4, 5]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[7, 0, 0],\n",
       "       [1, 2, 0],\n",
       "       [0, 4, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.band_part(x, -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[7, 6, 9],\n",
       "       [0, 2, 3],\n",
       "       [0, 0, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.band_part(x, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[7, 6, 0],\n",
       "       [0, 2, 3],\n",
       "       [0, 0, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.linalg.band_part(x, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead)\n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "      Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable\n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "      Returns:\n",
    "        output, attention_weights\n",
    "      \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[[ 1,  2,  3],\n",
    "                  [ 4,  5,  6]],\n",
    "                 [[ 7,  8,  9],\n",
    "                  [10, 11, 12]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2, 3])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
       "array([[[ 1,  4],\n",
       "        [ 2,  5],\n",
       "        [ 3,  6]],\n",
       "\n",
       "       [[ 7, 10],\n",
       "        [ 8, 11],\n",
       "        [ 9, 12]]], dtype=int32)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(x, perm=[0, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # (batch_size, seq_len_q, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512)) # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 512])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 8, 60, 60])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point wise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n",
    "sample_encoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        # (batch_size, target_seq_len, d_model)\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "                                                  False, None, None)\n",
    "sample_decoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # add embedding and position encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, dff=2048, input_vocab_size=8500,\n",
    "                        maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "print (sample_encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "output, attn = sample_decoder(temp_input,\n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False, \n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target,\n",
    "                 rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
